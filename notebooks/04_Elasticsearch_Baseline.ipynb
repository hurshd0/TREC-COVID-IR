{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fast and Powerful Full-text search Engine - Elasticsearch \n",
    "\n",
    "Setup Elasticsearch Cloud instance by going to -> [Elastic cloud](https://cloud.elastic.co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ug7v899j</td>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02tnwd4m</td>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ejv2xln0</td>\n",
       "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
       "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2b73a28n</td>\n",
       "      <td>Role of endothelin-1 in lung disease</td>\n",
       "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9785vg6d</td>\n",
       "      <td>Gene expression in epithelial cells in respons...</td>\n",
       "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                              title  \\\n",
       "0  ug7v899j  Clinical features of culture-proven Mycoplasma...   \n",
       "1  02tnwd4m  Nitric oxide: a pro-inflammatory mediator in l...   \n",
       "2  ejv2xln0    Surfactant protein-D and pulmonary host defense   \n",
       "3  2b73a28n               Role of endothelin-1 in lung disease   \n",
       "4  9785vg6d  Gene expression in epithelial cells in respons...   \n",
       "\n",
       "                                            abstract  \n",
       "0  OBJECTIVE: This retrospective chart review des...  \n",
       "1  Inflammatory diseases of the respiratory tract...  \n",
       "2  Surfactant protein-D (SP-D) participates in th...  \n",
       "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...  \n",
       "4  Respiratory syncytial virus (RSV) and pneumoni...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CORD19_PATH = Path('../data/input/trec_cord19_v0.csv')\n",
    "\n",
    "def load_cord19(input_fpath: Path, dtype: str = 'csv', cols_to_keep: list = ['cord_uid', 'abstract'], index_col = 'cord_uid') -> pd.DataFrame:\n",
    "    \"\"\"Loads CORD19 data and returns it as pandas data frame\n",
    "    \"\"\"\n",
    "    if dtype == 'csv':\n",
    "        df = pd.read_csv(input_fpath, quotechar='\"', index_col=index_col, usecols=cols_to_keep)\n",
    "        # for each column\n",
    "        for col in df.columns:\n",
    "            # check if the columns contains string data\n",
    "            if pd.api.types.is_string_dtype(df[col]):\n",
    "                df[col] = df[col].str.strip() # removes front and end white spaces\n",
    "                df[col] = df[col].str.replace('\\s{2,}', ' ') # remove double or more white spaces\n",
    "                df[col] = df[col].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "    return df\n",
    "\n",
    "cord19 = load_cord19(CORD19_PATH, cols_to_keep = ['cord_uid', 'abstract', 'title'], index_col=None)\n",
    "cord19.dropna(subset=['abstract'], inplace=True)\n",
    "cord19.fillna('', inplace=True)\n",
    "cord19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cord_uid    0\n",
       "title       0\n",
       "abstract    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cord19.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Connect to ES Cloud instance and Test ES client connection**\n",
    "\n",
    "Upload `elastic.json` which contains \n",
    "\n",
    "```\n",
    "{\n",
    "\"user\": \"elastic\", \n",
    "\"password\": \"<password>\",\n",
    "\"cloud_id\": \"<cloud_id>\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"elastic.json\") as elastic_file:\n",
    "    ELASTIC_SETTINGS = json.loads(elastic_file.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is ES client connected ? - True\n"
     ]
    }
   ],
   "source": [
    "es_client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_SETTINGS[\"cloud_id\"],\n",
    "    http_auth=(ELASTIC_SETTINGS[\"user\"], ELASTIC_SETTINGS[\"password\"]),\n",
    ")\n",
    "print(f'Is ES client connected ? - {es_client.ping()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2. Load TF Hub model**\n",
    "\n",
    "We are interested to use sentence embeddings techniques to embed the scholarly texts. We will use the Universal Sentence Encoder from TensorFlow Hub. You can use any pretrained models that can be used to produce the representations such as Hugging Face Transformers, etc. Load the pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate embeddings for a piece of abstract as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surfactant protein-D (SP-D) participates in the innate response to inhaled microorganisms and organic antigens, and contributes to immune and inflammatory regulation within the lung. SP-D is synthesized and secreted by alveolar and bronchiolar epithelial cells, but is also expressed by epithelial cells lining various exocrine ducts and the mucosa of the gastrointestinal and genitourinary tracts. SP-D, a collagenous calcium-dependent lectin (or collectin), binds to surface glycoconjugates expressed by a wide variety of microorganisms, and to oligosaccharides associated with the surface of various complex organic antigens. SP-D also specifically interacts with glycoconjugates and other molecules expressed on the surface of macrophages, neutrophils, and lymphocytes. In addition, SP-D binds to specific surfactant-associated lipids and can influence the organization of lipid mixtures containing phosphatidylinositol in vitro. Consistent with these diverse in vitro activities is the observation that SP-D-deficient transgenic mice show abnormal accumulations of surfactant lipids, and respond abnormally to challenge with respiratory viruses and bacterial lipopolysaccharides. The phenotype of macrophages isolated from the lungs of SP-D-deficient mice is altered, and there is circumstantial evidence that abnormal oxidant metabolism and/or increased metalloproteinase expression contributes to the development of emphysema. The expression of SP-D is increased in response to many forms of lung injury, and deficient accumulation of appropriately oligomerized SP-D might contribute to the pathogenesis of a variety of human lung diseases.\n"
     ]
    }
   ],
   "source": [
    "sample_doc = cord19.iloc[2]['abstract']\n",
    "print(sample_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed([sample_doc])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the embedding generated for the piece of text is of size (1, 512). These vector representation will be stored into Elasticsearch using a dense_vector type including the original pieces of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3. Index Abstract Text and Sentence Embeddings into ElasticSearch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup index settings before creating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = 'abstract'\n",
    "INDEX_FILE = 'abstract_settings.json'\n",
    "\n",
    "def load_index_file(INDEX_FILE):\n",
    "    \"\"\"Loads index file\"\"\"\n",
    "    with open(INDEX_FILE, encoding='utf-8') as index_file:\n",
    "        source = index_file.read().strip()\n",
    "    return source\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Converts text to sentence embeddings\"\"\"\n",
    "    vectors = embed(text)\n",
    "    return [vector.numpy().tolist() for vector in vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mappings': {'_source': {'enabled': 'true'},\n",
      "              'dynamic': 'true',\n",
      "              'properties': {'abstract': {'type': 'text'},\n",
      "                             'title': {'fields': {'keyword': {'type': 'keyword'}},\n",
      "                                       'type': 'text'},\n",
      "                             'title_vector': {'dims': 512,\n",
      "                                              'type': 'dense_vector'}}},\n",
      " 'settings': {'number_of_replicas': 1, 'number_of_shards': 2}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(json.loads(load_index_file(INDEX_FILE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Create Elasticsearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Deleting existing abstract index...\n",
      "\t[*] Creating abstract index...\n",
      "\t[+] abstract index created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_index(es_client):\n",
    "    \"\"\" Creates an Elasticsearch index.\"\"\"\n",
    "    is_created = False\n",
    "    # Index settings\n",
    "    settings = load_index_file(INDEX_FILE)\n",
    "    try:\n",
    "        if es_client.indices.exists(INDEX_NAME):\n",
    "            es_client.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "            print(f'\\t[*] Deleting existing {INDEX_NAME} index...')\n",
    "        print(f'\\t[*] Creating {INDEX_NAME} index...')\n",
    "        es_client.indices.create(index=INDEX_NAME, body=settings)\n",
    "        is_created = True\n",
    "        print(f'\\t[+] {INDEX_NAME} index created successfully.')\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "        print(f'\\t[X] Failed to create {INDEX_NAME} index.')\n",
    "    return is_created\n",
    "create_index(es_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Index data in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_data(es_client, df, BATCH_SIZE=1000):\n",
    "    \"\"\" Indexs all the rows in data (python questions).\"\"\"\n",
    "    docs = []\n",
    "    count = 0\n",
    "    for _, row in df.iterrows():\n",
    "        json_object = {}\n",
    "        json_object['cord_uid'] = row['cord_uid']\n",
    "        json_object['title'] = row['title']\n",
    "        json_object['abstract'] = row['abstract']\n",
    "        docs.append(json_object)\n",
    "        count += 1\n",
    "        \n",
    "        if count % BATCH_SIZE == 0:\n",
    "            index_batch(docs)\n",
    "            docs = []\n",
    "            print('Indexed {} documents.'.format(count))\n",
    "    if docs:\n",
    "        index_batch(docs)\n",
    "        print('Indexed {} documents.'.format(count))\n",
    "    \n",
    "    es_client.indices.refresh(index=INDEX_NAME)\n",
    "    print(\"Done indexing.\")\n",
    "\n",
    "def index_batch(docs):\n",
    "    titles = [doc['title'] for doc in docs]\n",
    "    title_vectors = embed_text(titles)\n",
    "\n",
    "    requests = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        request = doc\n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        request[\"title\"] = doc[\"title\"]\n",
    "        request[\"abstract\"] = doc['abstract']\n",
    "        request[\"title_vector\"] = title_vectors[i]\n",
    "        requests.append(request)\n",
    "    bulk(es_client, requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1000 documents.\n",
      "Indexed 2000 documents.\n",
      "Indexed 3000 documents.\n",
      "Indexed 4000 documents.\n",
      "Indexed 5000 documents.\n",
      "Indexed 6000 documents.\n",
      "Indexed 7000 documents.\n",
      "Indexed 8000 documents.\n",
      "Indexed 9000 documents.\n",
      "Indexed 10000 documents.\n",
      "Indexed 11000 documents.\n",
      "Indexed 12000 documents.\n",
      "Indexed 13000 documents.\n",
      "Indexed 14000 documents.\n",
      "Indexed 15000 documents.\n",
      "Indexed 16000 documents.\n",
      "Indexed 17000 documents.\n",
      "Indexed 18000 documents.\n",
      "Indexed 19000 documents.\n",
      "Indexed 20000 documents.\n",
      "Indexed 21000 documents.\n",
      "Indexed 22000 documents.\n",
      "Indexed 23000 documents.\n",
      "Indexed 24000 documents.\n",
      "Indexed 25000 documents.\n",
      "Indexed 26000 documents.\n",
      "Indexed 27000 documents.\n",
      "Indexed 28000 documents.\n",
      "Indexed 29000 documents.\n",
      "Indexed 30000 documents.\n",
      "Indexed 31000 documents.\n",
      "Indexed 32000 documents.\n",
      "Indexed 33000 documents.\n",
      "Indexed 34000 documents.\n",
      "Indexed 35000 documents.\n",
      "Indexed 36000 documents.\n",
      "Indexed 37000 documents.\n",
      "Indexed 38000 documents.\n",
      "Indexed 39000 documents.\n",
      "Indexed 40000 documents.\n",
      "Indexed 41000 documents.\n",
      "Indexed 42000 documents.\n",
      "Indexed 43000 documents.\n",
      "Indexed 44000 documents.\n",
      "Indexed 45000 documents.\n",
      "Indexed 46000 documents.\n",
      "Indexed 47000 documents.\n",
      "Indexed 48000 documents.\n",
      "Indexed 49000 documents.\n",
      "Indexed 50000 documents.\n",
      "Indexed 51000 documents.\n",
      "Indexed 52000 documents.\n",
      "Indexed 53000 documents.\n",
      "Indexed 54000 documents.\n",
      "Indexed 55000 documents.\n",
      "Indexed 56000 documents.\n",
      "Indexed 57000 documents.\n",
      "Indexed 58000 documents.\n",
      "Indexed 59000 documents.\n",
      "Indexed 60000 documents.\n",
      "Indexed 61000 documents.\n",
      "Indexed 62000 documents.\n",
      "Indexed 63000 documents.\n",
      "Indexed 64000 documents.\n",
      "Indexed 65000 documents.\n",
      "Indexed 66000 documents.\n",
      "Indexed 67000 documents.\n",
      "Indexed 68000 documents.\n",
      "Indexed 69000 documents.\n",
      "Indexed 70000 documents.\n",
      "Indexed 71000 documents.\n",
      "Indexed 72000 documents.\n",
      "Indexed 73000 documents.\n",
      "Indexed 74000 documents.\n",
      "Indexed 75000 documents.\n",
      "Indexed 76000 documents.\n",
      "Indexed 77000 documents.\n",
      "Indexed 78000 documents.\n",
      "Indexed 79000 documents.\n",
      "Indexed 80000 documents.\n",
      "Indexed 81000 documents.\n",
      "Indexed 82000 documents.\n",
      "Indexed 83000 documents.\n",
      "Indexed 84000 documents.\n",
      "Indexed 85000 documents.\n",
      "Indexed 86000 documents.\n",
      "Indexed 87000 documents.\n",
      "Indexed 88000 documents.\n",
      "Indexed 89000 documents.\n",
      "Indexed 90000 documents.\n",
      "Indexed 91000 documents.\n",
      "Indexed 92000 documents.\n",
      "Indexed 93000 documents.\n",
      "Indexed 94000 documents.\n",
      "Indexed 95000 documents.\n",
      "Indexed 96000 documents.\n",
      "Indexed 97000 documents.\n",
      "Indexed 98000 documents.\n",
      "Indexed 99000 documents.\n",
      "Indexed 100000 documents.\n",
      "Indexed 101000 documents.\n",
      "Indexed 101395 documents.\n",
      "Done indexing.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index_data(es_client, cord19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>question</th>\n",
       "      <th>query+question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic-id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>what is the origin of COVID-19</td>\n",
       "      <td>coronavirus origin what is the origin of COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus response to weather changes</td>\n",
       "      <td>how does the coronavirus respond to changes in...</td>\n",
       "      <td>coronavirus response to weather changes how do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coronavirus immunity</td>\n",
       "      <td>will SARS-CoV2 infected people develop immunit...</td>\n",
       "      <td>coronavirus immunity will SARS-CoV2 infected p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how do people die from the coronavirus</td>\n",
       "      <td>what causes death from Covid-19?</td>\n",
       "      <td>how do people die from the coronavirus what ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>animal models of COVID-19</td>\n",
       "      <td>what drugs have been active against SARS-CoV o...</td>\n",
       "      <td>animal models of COVID-19 what drugs have been...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            query  \\\n",
       "topic-id                                            \n",
       "1                              coronavirus origin   \n",
       "2         coronavirus response to weather changes   \n",
       "3                            coronavirus immunity   \n",
       "4          how do people die from the coronavirus   \n",
       "5                       animal models of COVID-19   \n",
       "\n",
       "                                                   question  \\\n",
       "topic-id                                                      \n",
       "1                            what is the origin of COVID-19   \n",
       "2         how does the coronavirus respond to changes in...   \n",
       "3         will SARS-CoV2 infected people develop immunit...   \n",
       "4                          what causes death from Covid-19?   \n",
       "5         what drugs have been active against SARS-CoV o...   \n",
       "\n",
       "                                             query+question  \n",
       "topic-id                                                     \n",
       "1         coronavirus origin what is the origin of COVID-19  \n",
       "2         coronavirus response to weather changes how do...  \n",
       "3         coronavirus immunity will SARS-CoV2 infected p...  \n",
       "4         how do people die from the coronavirus what ca...  \n",
       "5         animal models of COVID-19 what drugs have been...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_queries(input_fpath: Path, dtype: str = 'csv', cols_to_keep=['topic-id', 'query', 'question'], index_col=['topic-id']) -> pd.DataFrame:\n",
    "    \"\"\"Loads queries file and returns it as pandas data frame\n",
    "    \"\"\"\n",
    "    if dtype == 'csv':\n",
    "        df = pd.read_csv(input_fpath, quotechar='\"', index_col=index_col, usecols=cols_to_keep)\n",
    "        # for each column\n",
    "        for col in df.columns:\n",
    "            # check if the columns contains string data\n",
    "            if pd.api.types.is_string_dtype(df[col]):\n",
    "                df[col] = df[col].str.strip() # removes front and end white spaces\n",
    "                df[col] = df[col].str.replace('\\s{2,}', ' ') # remove double or more white spaces\n",
    "    return df\n",
    "\n",
    "QUERY_FPATH = Path('../data/CORD-19/CORD-19/topics-rnd3.csv')\n",
    "query_df = load_queries(QUERY_FPATH)\n",
    "query_df['query+question'] = query_df['query'] + ' ' + query_df['question']\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Query\n",
    "\n",
    "Let's write some helper functions below to query newly built Elastic Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_on_abstract(query, qid, run_name, es_client, top_k=10, verbose=False):\n",
    "    \"\"\" Searches the query and finds the best matches using elasticsearch.\"\"\"\n",
    "    # 1. create trec-covid template\n",
    "    template = \"{} Q0 {} {} {:.6f} {}\\n\"\n",
    "    # 2. create ES search query\n",
    "    search = {\n",
    "        \"size\": top_k, \n",
    "        \"query\": {\"match\": {\"abstract\": query}},\n",
    "        \"_source\": {\"includes\": [\"cord_uid\", \"title\"]}\n",
    "    }\n",
    "    response = es_client.search(\n",
    "        index=INDEX_NAME,\n",
    "        body=json.dumps(search)\n",
    "    )\n",
    "    ranked_lists = []\n",
    "    for rank, hit in enumerate(response[\"hits\"][\"hits\"]):\n",
    "        cord_uid = hit[\"_source\"][\"cord_uid\"]\n",
    "        score = hit[\"_score\"]\n",
    "        title = hit[\"_source\"][\"title\"]\n",
    "        ranked_lists.append(template.format(qid, cord_uid, rank+1, score, run_name))\n",
    "        if verbose:\n",
    "            print(\"\\tcord_id: {}\".format(cord_uid))\n",
    "            print(\"\\ttitle: {}\".format(title))\n",
    "            print(\"\\tscore: {}\".format(score))\n",
    "            print()\n",
    "    return ranked_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcord_id: 8ccl9aui\n",
      "\ttitle: Mosaic evolution of the severe acute respiratory syndrome coronavirus.\n",
      "\tscore: 9.693761\n",
      "\n",
      "\tcord_id: llv3cvdr\n",
      "\ttitle: Using the spike protein feature to predict infection risk and monitor the evolutionary dynamic of coronavirus\n",
      "\tscore: 9.092906\n",
      "\n",
      "\tcord_id: 4dtk1kyh\n",
      "\ttitle: Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using Artificial Intelligence\n",
      "\tscore: 8.979626\n",
      "\n",
      "\tcord_id: ab757i3f\n",
      "\ttitle: Emergence of a Novel Coronavirus, Severe Acute Respiratory Syndrome Coronavirus 2: Biology and Therapeutic Options\n",
      "\tscore: 8.883345\n",
      "\n",
      "\tcord_id: d6by9p41\n",
      "\ttitle: Emergence of a Novel Coronavirus, Severe Acute Respiratory Syndrome Coronavirus 2: Biology and Therapeutic Options\n",
      "\tscore: 8.883345\n",
      "\n",
      "\tcord_id: u65mey2z\n",
      "\ttitle: Classical Coronaviruses\n",
      "\tscore: 8.824791\n",
      "\n",
      "\tcord_id: hfkzu18p\n",
      "\ttitle: SARS-CoV-2 and COVID-19: The most important research questions\n",
      "\tscore: 8.689409\n",
      "\n",
      "\tcord_id: uqls3p01\n",
      "\ttitle: Genomic variance of the 2019-nCoV coronavirus\n",
      "\tscore: 8.6462755\n",
      "\n",
      "\tcord_id: hewbl5yu\n",
      "\ttitle: Using direct immunofluorescence to detect coronaviruses in peritoneal in peritoneal and pleural effusions\n",
      "\tscore: 8.604961\n",
      "\n",
      "\tcord_id: zel9a3u6\n",
      "\ttitle: Genomic variance of the 2019-nCoV coronavirus\n",
      "\tscore: 8.585432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qid = 1\n",
    "query = query_df.loc[qid]['query']\n",
    "tmp = search_on_abstract(query, qid, 'elastic_search_baseline', es_client, top_k=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(out_fpath, query_df, query_txt_col, es_client, run_name, top_k=1000):\n",
    "    \"\"\"Writes ranked results from elastic search results to txt file.\"\"\"\n",
    "    with open(out_fpath, 'w', encoding='utf-8') as writer:\n",
    "        for idx, row in query_df.iterrows():\n",
    "            qid = idx\n",
    "            query = row[query_txt_col]\n",
    "            ranked_lists = search_on_abstract(query, qid, run_name, es_client, top_k=top_k)\n",
    "            writer.writelines(ranked_lists)\n",
    "    print(f\"Wrote file @ {out_fpath}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Search on `abstract` using `query` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file @ ../data/output/elasticsearch_baseline_abstract_query.txt\n",
      "\n",
      "CPU times: user 309 ms, sys: 8.75 ms, total: 318 ms\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_name = 'elasticsearch_baseline_abstract_query'\n",
    "query_txt_col = 'query'\n",
    "out_fpath = Path('../data/output') / f'{run_name}.txt'\n",
    "write_results(out_fpath, query_df, query_txt_col, es_client, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\telasticsearch_baseline_abstract_query\n",
      "num_q                 \tall\t40\n",
      "num_ret               \tall\t40000\n",
      "num_rel               \tall\t10001\n",
      "num_rel_ret           \tall\t3371\n",
      "map                   \tall\t0.1285\n",
      "gm_map                \tall\t0.0684\n",
      "Rprec                 \tall\t0.2087\n",
      "bpref                 \tall\t0.3039\n",
      "recip_rank            \tall\t0.7259\n",
      "iprec_at_recall_0.00  \tall\t0.7757\n",
      "iprec_at_recall_0.10  \tall\t0.3716\n",
      "iprec_at_recall_0.20  \tall\t0.2641\n",
      "iprec_at_recall_0.30  \tall\t0.1685\n",
      "iprec_at_recall_0.40  \tall\t0.1018\n",
      "iprec_at_recall_0.50  \tall\t0.0738\n",
      "iprec_at_recall_0.60  \tall\t0.0325\n",
      "iprec_at_recall_0.70  \tall\t0.0037\n",
      "iprec_at_recall_0.80  \tall\t0.0000\n",
      "iprec_at_recall_0.90  \tall\t0.0000\n",
      "iprec_at_recall_1.00  \tall\t0.0000\n",
      "P_5                   \tall\t0.5500\n",
      "P_10                  \tall\t0.5350\n",
      "P_15                  \tall\t0.4833\n",
      "P_20                  \tall\t0.4537\n",
      "P_30                  \tall\t0.4167\n",
      "P_100                 \tall\t0.2905\n",
      "P_200                 \tall\t0.2174\n",
      "P_500                 \tall\t0.1325\n",
      "P_1000                \tall\t0.0843\n",
      "recall_5              \tall\t0.0143\n",
      "recall_10             \tall\t0.0278\n",
      "recall_15             \tall\t0.0374\n",
      "recall_20             \tall\t0.0457\n",
      "recall_30             \tall\t0.0625\n",
      "recall_100            \tall\t0.1405\n",
      "recall_200            \tall\t0.2007\n",
      "recall_500            \tall\t0.2923\n",
      "recall_1000           \tall\t0.3604\n",
      "infAP                 \tall\t0.1285\n",
      "gm_bpref              \tall\t0.2310\n",
      "Rprec_mult_0.20       \tall\t0.3779\n",
      "Rprec_mult_0.40       \tall\t0.3093\n",
      "Rprec_mult_0.60       \tall\t0.2588\n",
      "Rprec_mult_0.80       \tall\t0.2311\n",
      "Rprec_mult_1.00       \tall\t0.2087\n",
      "Rprec_mult_1.20       \tall\t0.1898\n",
      "Rprec_mult_1.40       \tall\t0.1753\n",
      "Rprec_mult_1.60       \tall\t0.1621\n",
      "Rprec_mult_1.80       \tall\t0.1499\n",
      "Rprec_mult_2.00       \tall\t0.1398\n",
      "utility               \tall\t-831.4500\n",
      "11pt_avg              \tall\t0.1629\n",
      "binG                  \tall\t0.0770\n",
      "G                     \tall\t0.0654\n",
      "ndcg                  \tall\t0.3427\n",
      "ndcg_rel              \tall\t0.3345\n",
      "Rndcg                 \tall\t0.2841\n",
      "ndcg_cut_5            \tall\t0.4700\n",
      "ndcg_cut_10           \tall\t0.4569\n",
      "ndcg_cut_15           \tall\t0.4261\n",
      "ndcg_cut_20           \tall\t0.4046\n",
      "ndcg_cut_30           \tall\t0.3767\n",
      "ndcg_cut_100          \tall\t0.2898\n",
      "ndcg_cut_200          \tall\t0.2704\n",
      "ndcg_cut_500          \tall\t0.3025\n",
      "ndcg_cut_1000         \tall\t0.3427\n",
      "map_cut_5             \tall\t0.0117\n",
      "map_cut_10            \tall\t0.0214\n",
      "map_cut_15            \tall\t0.0286\n",
      "map_cut_20            \tall\t0.0338\n",
      "map_cut_30            \tall\t0.0439\n",
      "map_cut_100           \tall\t0.0795\n",
      "map_cut_200           \tall\t0.1005\n",
      "map_cut_500           \tall\t0.1199\n",
      "map_cut_1000          \tall\t0.1285\n",
      "relative_P_5          \tall\t0.5500\n",
      "relative_P_10         \tall\t0.5350\n",
      "relative_P_15         \tall\t0.4833\n",
      "relative_P_20         \tall\t0.4537\n",
      "relative_P_30         \tall\t0.4167\n",
      "relative_P_100        \tall\t0.2924\n",
      "relative_P_200        \tall\t0.2549\n",
      "relative_P_500        \tall\t0.2934\n",
      "relative_P_1000       \tall\t0.3604\n",
      "success_1             \tall\t0.6000\n",
      "success_5             \tall\t0.9250\n",
      "success_10            \tall\t0.9500\n",
      "set_P                 \tall\t0.0843\n",
      "set_relative_P        \tall\t0.3604\n",
      "set_recall            \tall\t0.3604\n",
      "set_map               \tall\t0.0354\n",
      "set_F                 \tall\t0.1308\n",
      "num_nonrel_judged_ret \tall\t3198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_name = \"elasticsearch_baseline_abstract_query\"\n",
    "path_to_qrel_file = \"../data/qrels/qrels-covid_d3_j0.5-3.txt\"\n",
    "path_to_result_file = f\"../data/output/{run_name}.txt\"\n",
    "output_result_path = f\"../data/results/{run_name}_trec_eval.txt\"\n",
    "os.system(\"trec_eval -c -m all_trec {} {} > {}\".format(path_to_qrel_file, path_to_result_file, output_result_path))\n",
    "with open(output_result_path, encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Metrics**\n",
    "\n",
    "- `MAP` - 0.1285\n",
    "- `NDCG@10` - 0.4569\n",
    "- `P@5` - 0.5500\n",
    "- `R@1000` - 0.3604\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Search on `abstract` using `question + query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file @ ../data/output/elasticsearch_baseline_abstract_query_question.txt\n",
      "\n",
      "CPU times: user 314 ms, sys: 13.2 ms, total: 327 ms\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_name = 'elasticsearch_baseline_abstract_query_question'\n",
    "query_txt_col = 'query+question'\n",
    "out_fpath = Path('../data/output') / f'{run_name}.txt'\n",
    "write_results(out_fpath, query_df, query_txt_col, es_client, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\telasticsearch_baseline_abstract_query_question\n",
      "num_q                 \tall\t40\n",
      "num_ret               \tall\t40000\n",
      "num_rel               \tall\t10001\n",
      "num_rel_ret           \tall\t4113\n",
      "map                   \tall\t0.1679\n",
      "gm_map                \tall\t0.1180\n",
      "Rprec                 \tall\t0.2522\n",
      "bpref                 \tall\t0.3556\n",
      "recip_rank            \tall\t0.7923\n",
      "iprec_at_recall_0.00  \tall\t0.8408\n",
      "iprec_at_recall_0.10  \tall\t0.4761\n",
      "iprec_at_recall_0.20  \tall\t0.3490\n",
      "iprec_at_recall_0.30  \tall\t0.2489\n",
      "iprec_at_recall_0.40  \tall\t0.1536\n",
      "iprec_at_recall_0.50  \tall\t0.0769\n",
      "iprec_at_recall_0.60  \tall\t0.0339\n",
      "iprec_at_recall_0.70  \tall\t0.0040\n",
      "iprec_at_recall_0.80  \tall\t0.0000\n",
      "iprec_at_recall_0.90  \tall\t0.0000\n",
      "iprec_at_recall_1.00  \tall\t0.0000\n",
      "P_5                   \tall\t0.6300\n",
      "P_10                  \tall\t0.5975\n",
      "P_15                  \tall\t0.5817\n",
      "P_20                  \tall\t0.5538\n",
      "P_30                  \tall\t0.5158\n",
      "P_100                 \tall\t0.3552\n",
      "P_200                 \tall\t0.2650\n",
      "P_500                 \tall\t0.1618\n",
      "P_1000                \tall\t0.1028\n",
      "recall_5              \tall\t0.0172\n",
      "recall_10             \tall\t0.0309\n",
      "recall_15             \tall\t0.0443\n",
      "recall_20             \tall\t0.0557\n",
      "recall_30             \tall\t0.0770\n",
      "recall_100            \tall\t0.1692\n",
      "recall_200            \tall\t0.2406\n",
      "recall_500            \tall\t0.3497\n",
      "recall_1000           \tall\t0.4270\n",
      "infAP                 \tall\t0.1679\n",
      "gm_bpref              \tall\t0.3000\n",
      "Rprec_mult_0.20       \tall\t0.4661\n",
      "Rprec_mult_0.40       \tall\t0.3801\n",
      "Rprec_mult_0.60       \tall\t0.3242\n",
      "Rprec_mult_0.80       \tall\t0.2859\n",
      "Rprec_mult_1.00       \tall\t0.2522\n",
      "Rprec_mult_1.20       \tall\t0.2301\n",
      "Rprec_mult_1.40       \tall\t0.2097\n",
      "Rprec_mult_1.60       \tall\t0.1944\n",
      "Rprec_mult_1.80       \tall\t0.1806\n",
      "Rprec_mult_2.00       \tall\t0.1697\n",
      "utility               \tall\t-794.3500\n",
      "11pt_avg              \tall\t0.1985\n",
      "binG                  \tall\t0.0975\n",
      "G                     \tall\t0.0822\n",
      "ndcg                  \tall\t0.4132\n",
      "ndcg_rel              \tall\t0.4087\n",
      "Rndcg                 \tall\t0.3475\n",
      "ndcg_cut_5            \tall\t0.5704\n",
      "ndcg_cut_10           \tall\t0.5350\n",
      "ndcg_cut_15           \tall\t0.5227\n",
      "ndcg_cut_20           \tall\t0.5018\n",
      "ndcg_cut_30           \tall\t0.4695\n",
      "ndcg_cut_100          \tall\t0.3612\n",
      "ndcg_cut_200          \tall\t0.3321\n",
      "ndcg_cut_500          \tall\t0.3678\n",
      "ndcg_cut_1000         \tall\t0.4132\n",
      "map_cut_5             \tall\t0.0151\n",
      "map_cut_10            \tall\t0.0258\n",
      "map_cut_15            \tall\t0.0352\n",
      "map_cut_20            \tall\t0.0431\n",
      "map_cut_30            \tall\t0.0568\n",
      "map_cut_100           \tall\t0.1040\n",
      "map_cut_200           \tall\t0.1306\n",
      "map_cut_500           \tall\t0.1563\n",
      "map_cut_1000          \tall\t0.1679\n",
      "relative_P_5          \tall\t0.6300\n",
      "relative_P_10         \tall\t0.5975\n",
      "relative_P_15         \tall\t0.5817\n",
      "relative_P_20         \tall\t0.5538\n",
      "relative_P_30         \tall\t0.5158\n",
      "relative_P_100        \tall\t0.3586\n",
      "relative_P_200        \tall\t0.3089\n",
      "relative_P_500        \tall\t0.3511\n",
      "relative_P_1000       \tall\t0.4270\n",
      "success_1             \tall\t0.7000\n",
      "success_5             \tall\t0.8750\n",
      "success_10            \tall\t0.9500\n",
      "set_P                 \tall\t0.1028\n",
      "set_relative_P        \tall\t0.4270\n",
      "set_recall            \tall\t0.4270\n",
      "set_map               \tall\t0.0469\n",
      "set_F                 \tall\t0.1587\n",
      "num_nonrel_judged_ret \tall\t3881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_name = \"elasticsearch_baseline_abstract_query_question\"\n",
    "path_to_qrel_file = \"../data/qrels/qrels-covid_d3_j0.5-3.txt\"\n",
    "path_to_result_file = f\"../data/output/{run_name}.txt\"\n",
    "output_result_path = f\"../data/results/{run_name}_trec_eval.txt\"\n",
    "os.system(\"trec_eval -c -m all_trec {} {} > {}\".format(path_to_qrel_file, path_to_result_file, output_result_path))\n",
    "with open(output_result_path, encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Metrics**\n",
    "\n",
    "- `MAP` - 0.1679\n",
    "- `NDCG@10` - 0.5350\n",
    "- `P@5` - 0.6300\n",
    "- `R@1000` - 0.4270\n",
    "\n",
    "vs. best TF-IDF results,\n",
    "\n",
    "**TF-IDF `abstract` and `query+question`**\n",
    "\n",
    "- `MAP` - 0.1596\n",
    "- `NDCG@10` - 0.4311\n",
    "- `P@5` - 0.5450\n",
    "- `R@1000` - 0.4436\n",
    "\n",
    "Wow! Significant improvement over TF-IDF baseline, while recall slightly went down, precision improved and NDCG also improved. Next let's compare it against USE encoded `title vectors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Search using `title` vectors + `query + question`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinesearch_on_title_vectors(query, qid, run_name, es_client, top_k=10, verbose=False):\n",
    "    \"\"\" Searches using title vectors and finds the best matches using elasticsearch.\"\"\"\n",
    "    # 1. create trec-covid template\n",
    "    template = \"{} Q0 {} {} {:.6f} {}\\n\"\n",
    "    # 2. get query vector\n",
    "    query_vector = embed_text([query])[0]\n",
    "    # 3. create script query for cosine search\n",
    "    script_query = {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['title_vector']) + 1.0\",\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # 4. create ES search query\n",
    "    search = {\n",
    "        \"size\": top_k, \n",
    "        \"query\": script_query,\n",
    "        \"_source\": {\"includes\": [\"cord_uid\", \"title\"]}\n",
    "    }\n",
    "    # 5. search \n",
    "    response = es_client.search(\n",
    "        index=INDEX_NAME,\n",
    "        body=json.dumps(search)\n",
    "    )\n",
    "    # 6. parse results\n",
    "    ranked_lists = []\n",
    "    for rank, hit in enumerate(response[\"hits\"][\"hits\"]):\n",
    "        cord_uid = hit[\"_source\"][\"cord_uid\"]\n",
    "        score = hit[\"_score\"]\n",
    "        title = hit[\"_source\"][\"title\"]\n",
    "        ranked_lists.append(template.format(qid, cord_uid, rank+1, score, run_name))\n",
    "        if verbose:\n",
    "            print(\"\\tcord_id: {}\".format(cord_uid))\n",
    "            print(\"\\ttitle: {}\".format(title))\n",
    "            print(\"\\tscore: {}\".format(score))\n",
    "            print()\n",
    "    return ranked_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcord_id: dv9m19yk\n",
      "\ttitle: [What is the origin of SARS-CoV-2?]\n",
      "\tscore: 1.5443752\n",
      "\n",
      "\tcord_id: 8zwsi4nk\n",
      "\ttitle: Date of origin of the SARS coronavirus strains\n",
      "\tscore: 1.54244\n",
      "\n",
      "\tcord_id: a8voc4n9\n",
      "\ttitle: Date of origin of the SARS coronavirus strains.\n",
      "\tscore: 1.54244\n",
      "\n",
      "\tcord_id: 9jons9u9\n",
      "\ttitle: COVID-19 (Coronavirus)\n",
      "\tscore: 1.5143869\n",
      "\n",
      "\tcord_id: azdrlir3\n",
      "\ttitle: COVID-19 (Coronavirus)\n",
      "\tscore: 1.5143869\n",
      "\n",
      "\tcord_id: h8ahn8fw\n",
      "\ttitle: Origin and evolution of the 2019 novel coronavirus\n",
      "\tscore: 1.5045092\n",
      "\n",
      "\tcord_id: xdzbwa6z\n",
      "\ttitle: Origins of peptidases\n",
      "\tscore: 1.4890406\n",
      "\n",
      "\tcord_id: 4dtk1kyh\n",
      "\ttitle: Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using Artificial Intelligence\n",
      "\tscore: 1.4741515\n",
      "\n",
      "\tcord_id: q089uoz2\n",
      "\ttitle: The origin of HIV-1, the AIDS virus.\n",
      "\tscore: 1.4706244\n",
      "\n",
      "\tcord_id: 8na0nn5s\n",
      "\ttitle: The origin of HIV-1, the AIDS virus\n",
      "\tscore: 1.4706244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qid = 1\n",
    "query = query_df.loc[qid]['query+question']\n",
    "tmp = cosinesearch_on_title_vectors(query, qid, 'elastic_search_baseline', es_client, top_k=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(out_fpath, query_df, query_txt_col, es_client, run_name, top_k=1000):\n",
    "    \"\"\"Writes ranked results from elastic search results to txt file.\"\"\"\n",
    "    with open(out_fpath, 'w', encoding='utf-8') as writer:\n",
    "        for idx, row in query_df.iterrows():\n",
    "            qid = idx\n",
    "            query = row[query_txt_col]\n",
    "            ranked_lists = cosinesearch_on_title_vectors(query, qid, run_name, es_client, top_k=top_k)\n",
    "            writer.writelines(ranked_lists)\n",
    "    print(f\"Wrote file @ {out_fpath}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file @ ../data/output/elasticsearch_baseline_title_vectors_query_question.txt\n",
      "\n",
      "CPU times: user 7.2 s, sys: 2.17 s, total: 9.37 s\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_name = 'elasticsearch_baseline_title_vectors_query_question'\n",
    "query_txt_col = 'query+question'\n",
    "out_fpath = Path('../data/output') / f'{run_name}.txt'\n",
    "write_results(out_fpath, query_df, query_txt_col, es_client, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\telasticsearch_baseline_title_vectors_query_question\n",
      "num_q                 \tall\t40\n",
      "num_ret               \tall\t40000\n",
      "num_rel               \tall\t10001\n",
      "num_rel_ret           \tall\t2212\n",
      "map                   \tall\t0.0518\n",
      "gm_map                \tall\t0.0255\n",
      "Rprec                 \tall\t0.1180\n",
      "bpref                 \tall\t0.2092\n",
      "recip_rank            \tall\t0.6408\n",
      "iprec_at_recall_0.00  \tall\t0.6762\n",
      "iprec_at_recall_0.10  \tall\t0.1839\n",
      "iprec_at_recall_0.20  \tall\t0.0830\n",
      "iprec_at_recall_0.30  \tall\t0.0338\n",
      "iprec_at_recall_0.40  \tall\t0.0114\n",
      "iprec_at_recall_0.50  \tall\t0.0018\n",
      "iprec_at_recall_0.60  \tall\t0.0000\n",
      "iprec_at_recall_0.70  \tall\t0.0000\n",
      "iprec_at_recall_0.80  \tall\t0.0000\n",
      "iprec_at_recall_0.90  \tall\t0.0000\n",
      "iprec_at_recall_1.00  \tall\t0.0000\n",
      "P_5                   \tall\t0.3950\n",
      "P_10                  \tall\t0.3400\n",
      "P_15                  \tall\t0.3117\n",
      "P_20                  \tall\t0.2925\n",
      "P_30                  \tall\t0.2600\n",
      "P_100                 \tall\t0.1697\n",
      "P_200                 \tall\t0.1235\n",
      "P_500                 \tall\t0.0816\n",
      "P_1000                \tall\t0.0553\n",
      "recall_5              \tall\t0.0096\n",
      "recall_10             \tall\t0.0166\n",
      "recall_15             \tall\t0.0224\n",
      "recall_20             \tall\t0.0275\n",
      "recall_30             \tall\t0.0369\n",
      "recall_100            \tall\t0.0782\n",
      "recall_200            \tall\t0.1085\n",
      "recall_500            \tall\t0.1758\n",
      "recall_1000           \tall\t0.2347\n",
      "infAP                 \tall\t0.0518\n",
      "gm_bpref              \tall\t0.1741\n",
      "Rprec_mult_0.20       \tall\t0.2230\n",
      "Rprec_mult_0.40       \tall\t0.1757\n",
      "Rprec_mult_0.60       \tall\t0.1513\n",
      "Rprec_mult_0.80       \tall\t0.1315\n",
      "Rprec_mult_1.00       \tall\t0.1180\n",
      "Rprec_mult_1.20       \tall\t0.1081\n",
      "Rprec_mult_1.40       \tall\t0.1004\n",
      "Rprec_mult_1.60       \tall\t0.0928\n",
      "Rprec_mult_1.80       \tall\t0.0866\n",
      "Rprec_mult_2.00       \tall\t0.0814\n",
      "utility               \tall\t-889.4000\n",
      "11pt_avg              \tall\t0.0900\n",
      "binG                  \tall\t0.0443\n",
      "G                     \tall\t0.0419\n",
      "ndcg                  \tall\t0.2277\n",
      "ndcg_rel              \tall\t0.2290\n",
      "Rndcg                 \tall\t0.1831\n",
      "ndcg_cut_5            \tall\t0.3798\n",
      "ndcg_cut_10           \tall\t0.3377\n",
      "ndcg_cut_15           \tall\t0.3143\n",
      "ndcg_cut_20           \tall\t0.2961\n",
      "ndcg_cut_30           \tall\t0.2653\n",
      "ndcg_cut_100          \tall\t0.1875\n",
      "ndcg_cut_200          \tall\t0.1664\n",
      "ndcg_cut_500          \tall\t0.1921\n",
      "ndcg_cut_1000         \tall\t0.2277\n",
      "map_cut_5             \tall\t0.0081\n",
      "map_cut_10            \tall\t0.0125\n",
      "map_cut_15            \tall\t0.0155\n",
      "map_cut_20            \tall\t0.0177\n",
      "map_cut_30            \tall\t0.0214\n",
      "map_cut_100           \tall\t0.0332\n",
      "map_cut_200           \tall\t0.0393\n",
      "map_cut_500           \tall\t0.0473\n",
      "map_cut_1000          \tall\t0.0518\n",
      "relative_P_5          \tall\t0.3950\n",
      "relative_P_10         \tall\t0.3400\n",
      "relative_P_15         \tall\t0.3117\n",
      "relative_P_20         \tall\t0.2925\n",
      "relative_P_30         \tall\t0.2600\n",
      "relative_P_100        \tall\t0.1709\n",
      "relative_P_200        \tall\t0.1434\n",
      "relative_P_500        \tall\t0.1767\n",
      "relative_P_1000       \tall\t0.2347\n",
      "success_1             \tall\t0.5500\n",
      "success_5             \tall\t0.7250\n",
      "success_10            \tall\t0.7500\n",
      "set_P                 \tall\t0.0553\n",
      "set_relative_P        \tall\t0.2347\n",
      "set_recall            \tall\t0.2347\n",
      "set_map               \tall\t0.0148\n",
      "set_F                 \tall\t0.0856\n",
      "num_nonrel_judged_ret \tall\t1659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_name = \"elasticsearch_baseline_title_vectors_query_question\"\n",
    "path_to_qrel_file = \"../data/qrels/qrels-covid_d3_j0.5-3.txt\"\n",
    "path_to_result_file = f\"../data/output/{run_name}.txt\"\n",
    "output_result_path = f\"../data/results/{run_name}_trec_eval.txt\"\n",
    "os.system(\"trec_eval -c -m all_trec {} {} > {}\".format(path_to_qrel_file, path_to_result_file, output_result_path))\n",
    "with open(output_result_path, encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Metrics**\n",
    "\n",
    "- `MAP` - 0.0518\n",
    "- `NDCG@10` - 0.3377\n",
    "- `P@5` - 0.3950\n",
    "- `R@1000` - 0.2347\n",
    "\n",
    "Results are poor compared to Full-text search of Elasticsearch. One reason maybe it's `title` might be enough to get the proper context for query vector to form, also pre-trained embeddings usually have poor results on specific literature like COVID-19.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
