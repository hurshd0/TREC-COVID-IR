{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking using SciBERT\n",
    "\n",
    "Goal is to take best run so far i.e. elasticsearch on abstract, use SciBERT to rerank first `k` abstracts and evaluate the results again using TREC metric as usual.\n",
    "\n",
    "SciBERT will encode abstracts and queries into vectors of size : `[num_of_tokens, 786]`. If the sentence is longer than max. size of BERT, tokenizer will use slicing windows. We will use vectors of last hidden states of the BERT model as embedding to encode text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# ----------------- Classics -------------------- #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Pandas settings --------------- #\n",
    "# Removes rows and columns truncation of '...'\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# ------------------- NLP libs ---------------------- #\n",
    "import torch\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>title+abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cord_uid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ug7v899j</th>\n",
       "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02tnwd4m</th>\n",
       "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ejv2xln0</th>\n",
       "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
       "      <td>Surfactant protein-D and pulmonary host defens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b73a28n</th>\n",
       "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
       "      <td>Role of endothelin-1 in lung disease Endotheli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9785vg6d</th>\n",
       "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
       "      <td>Gene expression in epithelial cells in respons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   abstract  \\\n",
       "cord_uid                                                      \n",
       "ug7v899j  OBJECTIVE: This retrospective chart review des...   \n",
       "02tnwd4m  Inflammatory diseases of the respiratory tract...   \n",
       "ejv2xln0  Surfactant protein-D (SP-D) participates in th...   \n",
       "2b73a28n  Endothelin-1 (ET-1) is a 21 amino acid peptide...   \n",
       "9785vg6d  Respiratory syncytial virus (RSV) and pneumoni...   \n",
       "\n",
       "                                             title+abstract  \n",
       "cord_uid                                                     \n",
       "ug7v899j  Clinical features of culture-proven Mycoplasma...  \n",
       "02tnwd4m  Nitric oxide: a pro-inflammatory mediator in l...  \n",
       "ejv2xln0  Surfactant protein-D and pulmonary host defens...  \n",
       "2b73a28n  Role of endothelin-1 in lung disease Endotheli...  \n",
       "9785vg6d  Gene expression in epithelial cells in respons...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORD19_PATH = Path('../data/input/trec_cord19_v1.csv')\n",
    "\n",
    "def load_cord19(input_fpath: Path, dtype: str = 'csv', cols_to_keep: list = ['cord_uid', 'abstract'], index_col = 'cord_uid') -> pd.DataFrame:\n",
    "    \"\"\"Loads CORD19 data and returns it as pandas data frame\n",
    "    \"\"\"\n",
    "    if dtype == 'csv':\n",
    "        df = pd.read_csv(input_fpath, quotechar='\"', index_col=index_col, usecols=cols_to_keep)\n",
    "        # for each column\n",
    "        for col in df.columns:\n",
    "            # check if the columns contains string data\n",
    "            if pd.api.types.is_string_dtype(df[col]):\n",
    "                df[col] = df[col].str.strip() # removes front and end white spaces\n",
    "                df[col] = df[col].str.replace('\\s{2,}', ' ') # remove double or more white spaces\n",
    "                df[col] = df[col].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "    return df\n",
    "\n",
    "cord19 = load_cord19(CORD19_PATH, cols_to_keep = ['cord_uid', 'abstract', 'title+abstract'], index_col='cord_uid')\n",
    "cord19.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>question</th>\n",
       "      <th>query+question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic-id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>what is the origin of COVID-19</td>\n",
       "      <td>coronavirus origin what is the origin of COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus response to weather changes</td>\n",
       "      <td>how does the coronavirus respond to changes in...</td>\n",
       "      <td>coronavirus response to weather changes how do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coronavirus immunity</td>\n",
       "      <td>will SARS-CoV2 infected people develop immunit...</td>\n",
       "      <td>coronavirus immunity will SARS-CoV2 infected p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how do people die from the coronavirus</td>\n",
       "      <td>what causes death from Covid-19?</td>\n",
       "      <td>how do people die from the coronavirus what ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>animal models of COVID-19</td>\n",
       "      <td>what drugs have been active against SARS-CoV o...</td>\n",
       "      <td>animal models of COVID-19 what drugs have been...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            query  \\\n",
       "topic-id                                            \n",
       "1                              coronavirus origin   \n",
       "2         coronavirus response to weather changes   \n",
       "3                            coronavirus immunity   \n",
       "4          how do people die from the coronavirus   \n",
       "5                       animal models of COVID-19   \n",
       "\n",
       "                                                   question  \\\n",
       "topic-id                                                      \n",
       "1                            what is the origin of COVID-19   \n",
       "2         how does the coronavirus respond to changes in...   \n",
       "3         will SARS-CoV2 infected people develop immunit...   \n",
       "4                          what causes death from Covid-19?   \n",
       "5         what drugs have been active against SARS-CoV o...   \n",
       "\n",
       "                                             query+question  \n",
       "topic-id                                                     \n",
       "1         coronavirus origin what is the origin of COVID-19  \n",
       "2         coronavirus response to weather changes how do...  \n",
       "3         coronavirus immunity will SARS-CoV2 infected p...  \n",
       "4         how do people die from the coronavirus what ca...  \n",
       "5         animal models of COVID-19 what drugs have been...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_queries(input_fpath: Path, dtype: str = 'csv', cols_to_keep=['topic-id', 'query', 'question'], index_col=['topic-id']) -> pd.DataFrame:\n",
    "    \"\"\"Loads queries file and returns it as pandas data frame\n",
    "    \"\"\"\n",
    "    if dtype == 'csv':\n",
    "        df = pd.read_csv(input_fpath, quotechar='\"', index_col=index_col, usecols=cols_to_keep)\n",
    "        # for each column\n",
    "        for col in df.columns:\n",
    "            # check if the columns contains string data\n",
    "            if pd.api.types.is_string_dtype(df[col]):\n",
    "                df[col] = df[col].str.strip() # removes front and end white spaces\n",
    "                df[col] = df[col].str.replace('\\s{2,}', ' ') # remove double or more white spaces\n",
    "    return df\n",
    "\n",
    "QUERY_FPATH = Path('../data/CORD-19/CORD-19/topics-rnd3.csv')\n",
    "query_df = load_queries(QUERY_FPATH)\n",
    "query_df['query+question'] = query_df['query'] + ' ' + query_df['question']\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\telasticsearch_baseline_abstract_query_question\n",
      "num_q                 \tall\t40\n",
      "num_ret               \tall\t40000\n",
      "num_rel               \tall\t10001\n",
      "num_rel_ret           \tall\t4113\n",
      "map                   \tall\t0.1679\n",
      "gm_map                \tall\t0.1180\n",
      "Rprec                 \tall\t0.2522\n",
      "bpref                 \tall\t0.3556\n",
      "recip_rank            \tall\t0.7923\n",
      "iprec_at_recall_0.00  \tall\t0.8408\n",
      "iprec_at_recall_0.10  \tall\t0.4761\n",
      "iprec_at_recall_0.20  \tall\t0.3490\n",
      "iprec_at_recall_0.30  \tall\t0.2489\n",
      "iprec_at_recall_0.40  \tall\t0.1536\n",
      "iprec_at_recall_0.50  \tall\t0.0769\n",
      "iprec_at_recall_0.60  \tall\t0.0339\n",
      "iprec_at_recall_0.70  \tall\t0.0040\n",
      "iprec_at_recall_0.80  \tall\t0.0000\n",
      "iprec_at_recall_0.90  \tall\t0.0000\n",
      "iprec_at_recall_1.00  \tall\t0.0000\n",
      "P_5                   \tall\t0.6300\n",
      "P_10                  \tall\t0.5975\n",
      "P_15                  \tall\t0.5817\n",
      "P_20                  \tall\t0.5538\n",
      "P_30                  \tall\t0.5158\n",
      "P_100                 \tall\t0.3552\n",
      "P_200                 \tall\t0.2650\n",
      "P_500                 \tall\t0.1618\n",
      "P_1000                \tall\t0.1028\n",
      "recall_5              \tall\t0.0172\n",
      "recall_10             \tall\t0.0309\n",
      "recall_15             \tall\t0.0443\n",
      "recall_20             \tall\t0.0557\n",
      "recall_30             \tall\t0.0770\n",
      "recall_100            \tall\t0.1692\n",
      "recall_200            \tall\t0.2406\n",
      "recall_500            \tall\t0.3497\n",
      "recall_1000           \tall\t0.4270\n",
      "infAP                 \tall\t0.1679\n",
      "gm_bpref              \tall\t0.3000\n",
      "Rprec_mult_0.20       \tall\t0.4661\n",
      "Rprec_mult_0.40       \tall\t0.3801\n",
      "Rprec_mult_0.60       \tall\t0.3242\n",
      "Rprec_mult_0.80       \tall\t0.2859\n",
      "Rprec_mult_1.00       \tall\t0.2522\n",
      "Rprec_mult_1.20       \tall\t0.2301\n",
      "Rprec_mult_1.40       \tall\t0.2097\n",
      "Rprec_mult_1.60       \tall\t0.1944\n",
      "Rprec_mult_1.80       \tall\t0.1806\n",
      "Rprec_mult_2.00       \tall\t0.1697\n",
      "utility               \tall\t-794.3500\n",
      "11pt_avg              \tall\t0.1985\n",
      "binG                  \tall\t0.0975\n",
      "G                     \tall\t0.0822\n",
      "ndcg                  \tall\t0.4132\n",
      "ndcg_rel              \tall\t0.4087\n",
      "Rndcg                 \tall\t0.3475\n",
      "ndcg_cut_5            \tall\t0.5704\n",
      "ndcg_cut_10           \tall\t0.5350\n",
      "ndcg_cut_15           \tall\t0.5227\n",
      "ndcg_cut_20           \tall\t0.5018\n",
      "ndcg_cut_30           \tall\t0.4695\n",
      "ndcg_cut_100          \tall\t0.3612\n",
      "ndcg_cut_200          \tall\t0.3321\n",
      "ndcg_cut_500          \tall\t0.3678\n",
      "ndcg_cut_1000         \tall\t0.4132\n",
      "map_cut_5             \tall\t0.0151\n",
      "map_cut_10            \tall\t0.0258\n",
      "map_cut_15            \tall\t0.0352\n",
      "map_cut_20            \tall\t0.0431\n",
      "map_cut_30            \tall\t0.0568\n",
      "map_cut_100           \tall\t0.1040\n",
      "map_cut_200           \tall\t0.1306\n",
      "map_cut_500           \tall\t0.1563\n",
      "map_cut_1000          \tall\t0.1679\n",
      "relative_P_5          \tall\t0.6300\n",
      "relative_P_10         \tall\t0.5975\n",
      "relative_P_15         \tall\t0.5817\n",
      "relative_P_20         \tall\t0.5538\n",
      "relative_P_30         \tall\t0.5158\n",
      "relative_P_100        \tall\t0.3586\n",
      "relative_P_200        \tall\t0.3089\n",
      "relative_P_500        \tall\t0.3511\n",
      "relative_P_1000       \tall\t0.4270\n",
      "success_1             \tall\t0.7000\n",
      "success_5             \tall\t0.8750\n",
      "success_10            \tall\t0.9500\n",
      "set_P                 \tall\t0.1028\n",
      "set_relative_P        \tall\t0.4270\n",
      "set_recall            \tall\t0.4270\n",
      "set_map               \tall\t0.0469\n",
      "set_F                 \tall\t0.1587\n",
      "num_nonrel_judged_ret \tall\t3881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_name = \"elasticsearch_baseline_abstract_query_question\"\n",
    "path_to_qrel_file = \"../data/qrels/qrels-covid_d3_j0.5-3.txt\"\n",
    "path_to_result_file = f\"../data/output/{run_name}.txt\"\n",
    "output_result_path = f\"../data/results/{run_name}_trec_eval.txt\"\n",
    "os.system(\"trec_eval -c -m all_trec {} {} > {}\".format(path_to_qrel_file, path_to_result_file, output_result_path))\n",
    "with open(output_result_path, encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6549"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path_to_result_file, 'r') as f:\n",
    "    es_results = f.readlines()\n",
    "\n",
    "set_of_top_k_uid_all_topics = set()\n",
    "for line in es_results:\n",
    "    qid, _, uid, rank, _, _ = line.strip().split()\n",
    "    if int(rank) <= 200:\n",
    "        set_of_top_k_uid_all_topics.add(uid)\n",
    "\n",
    "# So for each 40 topics we get 40 * 200 ~ 8000 doc ids\n",
    "len(set_of_top_k_uid_all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6549 entries, keaxietu to 3twud97m\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   abstract        6549 non-null   object\n",
      " 1   title+abstract  6549 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 153.5+ KB\n"
     ]
    }
   ],
   "source": [
    "filtered_cord19 = cord19.loc[set_of_top_k_uid_all_topics]\n",
    "filtered_cord19.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6549, 40)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_dict = filtered_cord19['title+abstract'].to_dict()\n",
    "query_dict = query_df['query+question'].to_dict()\n",
    "len(abstracts_dict), len(query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SciBert (alternative: monologg/biobert_v1.1_pubmed)\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=False)\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 510\n",
    "\n",
    "def extract_scibert(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Encode text to vectors\n",
    "        text -  string to be encoded\n",
    "        tokenizer - BertTokenizer object\n",
    "        model - BertModel object\n",
    "        return - tensor of size [num_tokens, 768] (last hidden state of BERT)\n",
    "    \"\"\"\n",
    "    \n",
    "    text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    text_words = tokenizer.convert_ids_to_tokens(text_ids[0])[1:-1]\n",
    "\n",
    "    n_chunks = int(np.ceil(float(text_ids.size(1))/CHUNK_SIZE))\n",
    "    states = []\n",
    "    \n",
    "    for ci in range(n_chunks):\n",
    "        text_ids_ = text_ids[0, 1+ci*CHUNK_SIZE:1+(ci+1)*CHUNK_SIZE]  \n",
    "        text_ids_ = torch.cat([text_ids[0, 0].unsqueeze(0), text_ids_])\n",
    "        if text_ids_[-1] != text_ids[0, -1]:\n",
    "            text_ids_ = torch.cat([text_ids_, text_ids[0,-1].unsqueeze(0)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = model(text_ids_.unsqueeze(0))[0] # last hidden states\n",
    "            state = state[:, 1:-1, :]\n",
    "        states.append(state)\n",
    "\n",
    "    state = torch.cat(states, axis=1)\n",
    "    return state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_abstract_query_narrative_and_save(query_dict, abstracts_dict, extract_scibert, tokenizer, model, fname, **kwargs):\n",
    "    \"\"\"\n",
    "    Encode topics and abstracts using given encoding function, save it to path_to_output\n",
    "        query_dict\n",
    "        abstracts_dict\n",
    "        extract_scibert - BERT encoding function, \n",
    "            input: text, tokenizer, model **kwargs, \n",
    "            output  tensor of size [num, 768]\n",
    "        tokenizer - to pass to extract_scibert\n",
    "        model - to pass to extract_scibert\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode abstract\n",
    "    encoded_abstract = dict()\n",
    "\n",
    "    for uid, text in tqdm(abstracts_dict.items()):\n",
    "        encoded_abstract[uid] = extract_scibert(text, tokenizer, model, **kwargs)\n",
    "\n",
    "    # encode queries\n",
    "    encoded_queries = dict()\n",
    "\n",
    "    for qid, query in topics.items():\n",
    "        encoded_queries[qid] = extract_scibert(query, tokenizer, model)\n",
    "        \n",
    "    # save for future use\n",
    "    bert_vectors = {\n",
    "            \"abstract\": encoded_abstract, \n",
    "            \"query\": encoded_queries\n",
    "    }\n",
    "\n",
    "    with open(\"../data/embeddings/\" + fname, \"wb\") as f:\n",
    "        pickle.dump(bert_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 809/6549 [19:28<1:28:01,  1.09it/s]"
     ]
    }
   ],
   "source": [
    "encode_abstract_query_narrative_and_save(\n",
    "    query_dict,\n",
    "    abstracts_dict,\n",
    "    extract_scibert,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    \"scibert.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open(\"../data/embeddings/scibert.pkl\", \"rb\") as f:\n",
    "    bert_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_match(state1, state2):\n",
    "    state1 = state1 / torch.sqrt((state1 ** 2).sum(1, keepdims=True))\n",
    "    state2 = state2 / torch.sqrt((state2 ** 2).sum(1, keepdims=True))\n",
    "    sim = (state1.unsqueeze(1) * state2.unsqueeze(0)).sum(-1)\n",
    "    return sim\n",
    "\n",
    "def rerank(topics, search_run, abstracts_dict, top_k, run_name, topics_field, bert_vectors):\n",
    "    \"\"\"\n",
    "    Rerank the original run and save the reranked run in path_to_reranked_run\n",
    "        query_dict -  dict where key=qid and value=query+question\n",
    "        abstracts_dict - dict where key=uid and value=abstract\n",
    "        search_run - python list of previous search runs\n",
    "        top_k - to rerank, the rest will remain same\n",
    "        run_name - reranked run name\n",
    "        bert_vectors dict - key = qid, uid and value = vectors\n",
    "    \"\"\"\n",
    "    rerank = defaultdict(list)  # first k hits\n",
    "    keeprank = defaultdict(list) # k+1 to 1000 hits\n",
    "   \n",
    "    encoded_queries = bert_vectors[\"query\"] \n",
    "    encoded_abstract = bert_vectors[\"abstract\"]\n",
    "\n",
    "    # calculate similarity\n",
    "    for line in search_run:\n",
    "        qid, _, uid, j, score, _ = line.strip().split()\n",
    "        if len(rerank[qid]) < top_k:\n",
    "            if not abstracts_dict[uid]:\n",
    "                continue # Some uid don't have abstract. But why they show up in Anserini run?\n",
    "\n",
    "            _, _, enc_abs = encoded_abstract[abstracts_dict[uid]]\n",
    "            _, _, enc_query = encoded_queries[query_dict[qid]]\n",
    "            sim = cross_match(enc_query, enc_abs)\n",
    "\n",
    "            rel_score = torch.max(sim).item()\n",
    "            rerank[qid].append([uid, rel_score])\n",
    "\n",
    "        elif len(rerank[qid]) >= top_k and len(keeprank[qid]) < 1000 - top_k: \n",
    "            keeprank[qid].append([uid, score, j])\n",
    "\n",
    "\n",
    "    # create reranked run and save to path_to_reranked_run\n",
    "    template = \"{} Q0 {} {} {} {}\"\n",
    "    run = list()\n",
    "\n",
    "    for qid in rerank:\n",
    "        rank = 1\n",
    "        for uid, score in sorted(rerank[qid], key=lambda x:-x[1]):\n",
    "            run.append(template.format(qid, uid, rank, score + 10, run_name))\n",
    "            rank += 1\n",
    "\n",
    "        for uid, score, j in keeprank[qid]:\n",
    "            run.append(template.format(qid, uid, rank, score, run_name))\n",
    "            rank += 1\n",
    "            \n",
    "        assert rank == 1001 # if no bugs, each topic will have at most 1000 uid (can be less if original run has less)\n",
    "\n",
    "    with open(\"../data/output/{run_name}.txt\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
